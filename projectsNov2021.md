# Project Offerings

# Lovelab ([bradlove.org](http://bradlove.org)), a computational cognitive science lab directed by Prof Brad Love.


We are looking for enthusiastic and motivated students who want to collaborate with Prof Brad Love and members of his [lab](http://bradlove.org). We aim to improve performance of neural network models and make them more human-like by using insights from neuroscience. Students with approrpriate skills with a strong interest in training and evaluating convolutional neural networks should contact Prof Love, b.love@ucl.ac.uk.

## Prerequisites

I am getting some emails from enthusiastic students (always great) who unfortunately do not have the backgrounds to complete the projects below. Students are going to need an undergraduate degree in computer science degree or closely related field. Students must be proficient in Python and familiar with TensorFlow and/or PyTorch. Students should also be comfortable with linear algebra, multivariate calculus, version control (git), and version control repositories (GitHub). These are not concepts that we are going to teach or tasks my lab will do for you, but the starting points for your project work. We are here to do MSc level research with you, not provide undergraduate training in computer science. There simply isn't enough time to master these basic concepts and also do the project work.

## 1. Aligning conceptual spaces
The semantic distribution hypothesis posits that a word's meaning is captured by its patterns of co-occurrence with other words - or in other words, that 'you shall know a word by the company it keeps'. These patterns of co-occurrence form the basis of word embeddings, which underpin modern NLP applications. Structural relationships within the embedding spaces of different languages have been successfully leveraged to enhance machine translation techniques. But what might the differences in structural relationships between languages reveal about the meaning of concepts between cultures? We are interested in applying alignment techniques to real-world data to identify areas of conceptual incongruence, and in exploring the hypothesis that poor alignment in conceptual space reflects human notions of translational inadequacy.

## 2. Relating the brain to artificial neural networks through embedding spaces
Artificial neural networks (ANN) are useful tools both for doing supervised learning and for creating embedding spaces. Furthermore, they have even proved to have predictive power for the visual ventral stream in the human brain. This project seeks to reverse the direction of influence by using neural embedding spaces derived from fMRI data (e.g., BOLD5000) to improve the interpretability of ANN solutions. Thus, the goal is to tune ANN solutions to be more brain-like by leveraging embedding spaces inferred from fMRI and other data. One offshot of this project is introducing topological constraints into neural networks to make them more brain-like (and hopefully perform better too).

## 3. Attentional mechanisms for convolutional neural networks (No longer available, FULL)
Recent work has related Convolutional Neural Networks (CNN) to the human visual system. However, standard CNNs lacks an essential human ability, namely the ability to selectively attend to information according to the task context. This project aims to endow CNNs with this ability. For example, when searching for one's dog, one may be more sensitive to features that reflect the colour and texture of the dog's fur irrespective of location. The general project aim is to both improve machine learning models and to create better models of human performance. The project will be extending on this [recent work](https://arxiv.org/abs/2002.02342) in the lab.

## 4. Heuristic regularisers for neural networks
[Recent work](https://arxiv.org/abs/2010.02610) in the lab has focused on generalizing ridge regression with robust priors derived from decision heuristics that humans use. Instead of centering a model's solution on the zero vector, heuristics suggest other sensible targets. Our approach is readily extended to the regularisation of neural networks. Artificial neural networks (ANNs) are typically regularised with an L1 or L2 norm to avoid overfitting. This project involves the construction of human-like heuristics as priors on the weights of ANNs. The goal is to selectively insert such priors at certain layers in the network to track improvements in performance, robustness, and interpretability.

## 5. Learning to learn in neural networks (FULL)
Convolutional neural networks can learn to classify a wide range of images from a set of training data. However equal attention is usually paid to all categories during training, despite some classifications being more difficult than others. Further, human input is required to optimise the parameters of the network. This project will involve implementing a supplementary network which learns through feedback from the main model, which classes to select for training, and which parameters lead to optimal performance. The goal is to allow neural networks more control over their own training, and to choose the best learning strategies.

