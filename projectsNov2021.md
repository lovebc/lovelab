# Project Offerings

# Lovelab ([bradlove.org](http://bradlove.org)), a computational cognitive science lab directed by Prof Brad Love.


We are looking for enthusiastic and motivated students who want to collaborate with Prof Brad Love and members of his [lab](http://bradlove.org). We aim to improve performance of neural network models and make them more human-like by using insights from neuroscience. Students with approrpriate skills with a strong interest in training and evaluating convolutional neural networks should contact Prof Love, b.love@ucl.ac.uk.

## Prerequisites

I am getting some emails from enthusiastic students (always great) who unfortunately do not have the backgrounds to complete the projects below. Students are going to need an undergraduate degree in computer science degree or closely related field. Students must be proficient in Python and familiar with TensorFlow and/or PyTorch. Students should also be comfortable with linear algebra, multivariate calculus, version control (git), and version control repositories (GitHub). These are not concepts that we are going to teach or tasks my lab will do for you, but the starting points for your project work. We are here to do MSc level research with you, not provide undergraduate training in computer science. There simply isn't enough time to master these basic concepts and also do the project work.

## 1. Attentional mechanisms for convolutional neural networks (Open)
Recent work has related Convolutional Neural Networks (CNN) to the human visual system. However, standard CNNs lacks an essential human ability, namely the ability to selectively attend to information according to the task context. This project aims to endow CNNs with this ability. For example, when searching for one's dog, one may be more sensitive to features that reflect the colour and texture of the dog's fur irrespective of location. The general project aim is to both improve machine learning models and to create better models of human performance. The project will be extending on this [recent work](https://arxiv.org/abs/2002.02342) in the lab.

## 2. Comparing natural language translations using transformers (Open)
Literary translation is the art of transferring meaning across linguistic systems. Translations of many works are contested on the extent to which they capture the text's meaning in the original language. This project aims to explore how well meaning is conveyed within contested translations by using the structural relationships between words in embedding spaces. Word embeddings have been shown to reliably capture important elements of word meaning at scale, and form the basis of many modern day NLP applications. In turn, the similarity of structural relationships across multilingual embedding spaces has been leveraged successfully in  machine translation. Here, we aim to explore what dissimilarity in these structural relationships can reveal about differences in meaning. There is scope to apply transformer-based methods and centred kernel alignment to tackle this question. 

## 3. Preventing shortcut learning in deep convolutional neural networks (Open)
Deep convolutional neural networks (DCNNs) achieved notable success in image classification, sometimes reaching human-level performance. However, unlike human performance, impressive accuracies of DCNNs often drop significantly when training and test datasets are drawn from different distributions. This poor out-of-sample generalisation is often related to the DCNNsâ€™ tendency to learn simplistic features, largely ignoring co-present more complex ones, even if a reliance on the latter would lead to a greater accuracy. Such shortcut features are often a consequence of spurious correlations in a training set, stemming from limitations in data collection, and do not capture invariant structures in the domain. The tendency to rely on shortcuts implies that modern DCNNs represent stimuli differently from the way humans do, even when, at first glance, a similar performance on the same tasks can be achieved. This project will aim at developing novel approaches to avoid shortcut reliance in DCNNs as well as at exploring potential reasons for it. Since the shortcut reliance is a prominent dissimilarity of human and DCNN learning, the project will focus on leveraging some characteristics of human cognition that might be relevant for learning complex representations: processing stimuli deeper than sufficient to succeed on an immediate task with a restricted dataset, facing the same stimuli in the context of several tasks, treating stimuli as examples of multiple categories, and others. Potential outcomes of a reduced shortcut reliance are related not only to more robust DCNNs but also to a better suitability of those for modelling human brain and cognition.

